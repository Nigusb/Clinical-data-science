---
title: "Practical on clustering"
author: "Leonard Wee"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Reminder of what we are going to look at

Recall that the ultimate goal of any kind of modelling process is to **derive a simpler low-dimensional representation** of a dataset.

For example, if we have a scatter of points on an ***x-y plot*** we are familiar to reduce all the complexity of the data into only **two** descriptors - it will be the INTERCEPT and the SLOPE of a straight line (of course we are assuming that a linear description is appropriate in the first place).

Over the top of this compressed description is what (we hope) contains only purely random NOISE. If we have done a good job as statisticians and modellers, then we might have some confidence that the compressed descriptors really do indeed capture the SIGNAL in the data.

  *  SIGNAL - patterns generated by the phenomenon that we are actually interested to understand;
  *  NOISE - random variation that we are not interested in.

### Recap two modelling approaches that may be complementary
![](./fig_label_vs_nonlabel_models.png)

## Get the data ready

Reminder that these are steps we have already seen before.

```{r a-much-nicer-install-and-load, message=FALSE, warning=FALSE}
source("handy_dandy_functions.R")

list.of.packages <- c("dplyr",
                      "magrittr",
                      "ggplot2", "GGally",
                      "lubridate") #a list of packages to use

installRequiredPackages(list.of.packages)
```

```{r open-covid-data}
pathToData <- './open_covid_data' #this means "where my markdown script is, there is a subfolder called open-covid-data"

rawOpenCovid <- read.csv( file.path(pathToData,'Covid_cleaned.csv') )

recodedOpenCovid <- dplyr::mutate(rawOpenCovid,
    #
    continent = as.factor(continent),
    #
    location = as.factor(location),
    #
    date = as.Date(date)
  )

recodedOpenCovid <- dplyr::select(recodedOpenCovid,
                                  continent,
                                  location,
                                  date,
                                  total_cases_per_million,
                                  total_deaths_per_million,
                                  population_density,
                                  median_age,
                                  aged_65_older,
                                  gdp_per_capita,
                                  extreme_poverty,
                                  cardiovasc_death_rate,
                                  hospital_beds_per_thousand,
                                  life_expectancy,
                                  human_development_index
                                  )

summary(recodedOpenCovid)
```
```{r last-recorded-date per location}
last_recorded_date_per_location <- with(recodedOpenCovid,
     aggregate(date, by=list(location), FUN=max)
     #this is a transform function that aggregates multiple dates into one - in this case the MAX date
     )

summary(last_recorded_date_per_location$x) #and everyone seems to have the same end date

length(last_recorded_date_per_location$Group.1) #which indicates 89 locations
```

```{r subset of data as of 01-10-2021}
subOpenCovid <- dplyr::filter(recodedOpenCovid,
              date == "2021-10-01") #taking only the last recorded totals

subOpenCovid %<>% dplyr::select(., -date) #this keeps everything except date

subOpenCovid %<>% dplyr::filter(., continent != "Oceania") #this keeps everything except oceania
#NB in data exploration I notice that Oceanie only has two location points
```

```{r scatter-2d-plot-cases-deaths-continents}
normalversion <- ggplot(subOpenCovid,
       aes(x=total_cases_per_million,
           y=total_deaths_per_million,
           colour = continent,
           shape = continent)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_shape_manual(values=c(15:19)) +
  scale_colour_brewer(palette = "Set1") +
  ggtitle("Cases and Deaths per million - by continent :") +
  theme_bw()

loglogversion <- ggplot(subOpenCovid,
       aes(x=total_cases_per_million,
           y=total_deaths_per_million,
           colour = continent,
           shape = continent)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_shape_manual(values=c(15:19)) +
  scale_colour_brewer(palette = "Set1") +
  ggtitle("Cases and Deaths per million - by continent :") +
  theme_bw() +
  scale_x_continuous(trans = 'log10') +
  scale_y_continuous(trans = 'log10')

normalversion

loglogversion
```

And again the "pairs" graphics from last time as a big overview :

```{r}
subOpenCovid <- dplyr::mutate(subOpenCovid,
                              #
                              death_per_case = total_deaths_per_million / total_cases_per_million
                              ) #normalize deaths to number of cases
ggpairs(
  subOpenCovid[,c(3:13)],
  upper = list(continuous = "density", combo = "box_no_facet"),
  lower = list(continuous = "points", combo = "dot_no_facet")
)
```

```{r}
ggplot(subOpenCovid,
       aes(x=total_cases_per_million,
           y=death_per_case,
           colour = continent,
           shape = continent)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_shape_manual(values=c(15:19)) +
  scale_colour_brewer(palette = "Set1") +
  ggtitle("Cases per million and chance of death per case - by continent :") +
  theme_bw() + 
  scale_x_continuous(trans = 'log10') +
  scale_y_continuous(trans = 'log10')
```

## Experimentations with clustering tendency and "similarity"

### Preparation steps

Remove the obvious deterministic dependency :

```{r drop-deaths-per-million}
subOpenCovid %<>% dplyr::select(., -total_deaths_per_million)
```

Take out the categorical variables for the time being :

```{r cluster-df}
clusterDF <- dplyr::select(subOpenCovid, -continent, -location)
```

Express as normalized scores (scaling), assuming everything should be converted to standardized z-statistic. This assumption may not be true for highly skewed distributions! But for now we ignore this obvious danger. In practice, we would alreayd have done quite a lot of data exploration and looked at the variance within each variable, so we should be able to make an informed choice about this.

```{r normalization}
clusterDF_scaled <- scale(clusterDF) #assuming everything should be converted to standardized z-statistic
#this assumption may not be true for highly skewed distributions! But for now we ignore this obvious danger.
#
```

The needed libraries for clustering analysis :

```{r clustering-packages, message=FALSE, warning=FALSE}
installRequiredPackages(
  c("cluster","factoextra")
)
```

### Visual estimation using a clustering (Hopkins') statistic

```{r print-hopkins-stat}
gradient.color <- list(low = "red",  high = "white")

hopkinsClustering <- get_clust_tendency(clusterDF_scaled,
                                        n = nrow(clusterDF_scaled)-1,
                                        gradient = gradient.color)

hopkinsClustering$hopkins_stat
```

```{r heatmap-hopkins-stat}
hopkinsClustering$plot
```

### Distance (or similarity) metric

In order to define clusters, we need a measurement of similarity. Mathematically, similarity is calculated as a type of "distance". We are used to think of distance in kilometres or miles, but it is also possible to propose it is the ***sum of squares of the difference between any two subjects' values***. In this case it is the classical Euclidean (flat space) distance but you could imagine an infinite variety of ways to define the similarity between two subjects etc.

```{r euclidean-distance-metric-example}
euclid.dist <- get_dist(clusterDF_scaled, method = "euclidean")

fviz_dist(euclid.dist, 
   gradient = list(low = "darkred", mid = "white", high = "darkblue"))
```

### Experimenting with different "distance" metrics

```{r manhattan-distance-metric-example}
manhattan.dist <- get_dist(clusterDF_scaled, method = "manhattan")

fviz_dist(manhattan.dist, 
   gradient = list(low = "darkred", mid = "white", high = "darkblue"))
```

```{r maximum-distance-metric-example}
maximum.dist <- get_dist(clusterDF_scaled, method = "maximum")

fviz_dist(maximum.dist, 
   gradient = list(low = "darkred", mid = "white", high = "darkblue"))
```

### Correlation-based distances

Correlation says that two subjects similar if their value over all variables are highly correlated, even if the actual values of one specific variable might be far apart in terms of Euclidean distance. A correlation distance of exactly zero happens when two subjects are perfectly correlated even if their values are widely different.

Example, patient Y has every single measurement that is perfectly triple the measurement of patient X. Then the correlation distance between X and Y is zero even though the absolute values of the measurements are widely different.

**Please note this difference for interpretation, because normally think that zero implies NO correlation**.

```{r correlation-distance-metric-pearson}
pearson.dist <- get_dist(clusterDF_scaled, method = "pearson")

fviz_dist(pearson.dist, 
   gradient = list(low = "darkred", mid = "white", high = "darkblue"))
```

```{r correlation-distance-metric-spearman}
spearman.dist <- get_dist(clusterDF_scaled, method = "spearman")

fviz_dist(spearman.dist, 
   gradient = list(low = "darkred", mid = "white", high = "darkblue"))
```

The balance of visual analysis suggests to us that, one way or other, there is quite a high clustering tendency in this dataset, so we might be able to find the clusters and try to interpret them.

## K-means clustering

I am going to start with just a bit of a guess.

```{r}
set.seed(1299) # Setting random number generator seed - could be useful if you want the same result each time
kmeans.re <- kmeans(clusterDF_scaled, centers = 4, nstart = 25)
kmeans.re
```

But there is a much more systematic way to search for the clusters, which you have seen previously. The slightly more subjective decision is where on the "elbow" to choose.

```{r elbow-method}
fviz_nbclust(clusterDF_scaled, kmeans, method = "wss") +
    #geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```

```{r}
kmeans.re <- kmeans(clusterDF_scaled, centers = 3, nstart = 25) #maybe 3 is fine?
membership <- kmeans.re$cluster

subOpenCovid$membership <- as.factor(membership)
```

Now re-doing the earlier plot, but instead of using the predefined continents, I'll use the cluster memberships assigned by K-Means:

```{r}
ggplot(subOpenCovid,
       aes(x=total_cases_per_million,
           y=death_per_case,
           colour = membership,
           shape = membership)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_shape_manual(values=c(15:17)) +
  scale_colour_brewer(palette = "Set1") +
  ggtitle("Cases per million and chance of death per case - cluster membership :") +
  theme_bw() + 
  scale_x_continuous(trans = 'log10') +
  scale_y_continuous(trans = 'log10')
```

### Heirarchical clustering (compare with K-means clustering)

One of the potential criticisms about K-means clustering if the reduced transparency or subjectivity when choosing how many clusters. It is harder to visualize what would happen if we defined more or fewer clusters other than to do it and try it.

In contrast hierarchical clustering is a **tree-based** representation of the data, which is also known as a "dendrogram". We will be able to see all the possible clusters at once, without having to define the number of clusters up front. However we need to decide which level to "cut" the tree, which then gives us the cluster memberships.

```{r}
dis <- dist(clusterDF_scaled, method = "euclidean")
res.hc.euclid <- hclust(dis,  method = "ward.D")

fviz_dend(res.hc.euclid, k = 4, # Cut in four groups
          cex = 0.5, # label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE # Add rectangle around groups
          )
```

```{r}
hc_membership <- cutree(res.hc.euclid, k = 4)

subOpenCovid$hc_membership <- as.factor(hc_membership)

ggplot(subOpenCovid,
       aes(x=total_cases_per_million,
           y=death_per_case,
           colour = hc_membership,
           shape = hc_membership)) +
  geom_point(size = 3, alpha = 0.5) +
  scale_shape_manual(values=c(15:18)) +
  scale_colour_brewer(palette = "Set1") +
  ggtitle("Cases per million and chance of death per case - heirachical membership :") +
  theme_bw() + 
  scale_x_continuous(trans = 'log10') +
  scale_y_continuous(trans = 'log10')
```

The separation between the clusters is not so obvious in this projected view of deaths per case versus total cases per million population. It is left as a bit of a data exploration experiment if you can find ***other data visualization views** that might be able to help interpret the underlying signal of the clusters.

## Re-visiting principal components

[An intuitive visual explanation of principal components analysis as a dimensionality REDUCTION tool](https://setosa.io/ev/principal-component-analysis/https://www.google.com)

```{r}
pca_DF <- prcomp(clusterDF_scaled)
```


```{r}
fviz_pca_ind(pca_DF, title = "PCA - Open Covid data - Continental Clusters", 
             habillage = subOpenCovid$continent,
             palette = "jco",
             geom = "point",
             ggtheme = theme_bw(),
             alpha.ind = 0.5,
             addEllipses = TRUE,
             legend = "right")
```

```{r}
fviz_pca_ind(pca_DF, title = "PCA - Open Covid data - KMeans Clusters (n=3)", 
             habillage = subOpenCovid$membership,
             palette = "jco",
             geom = "point",
             ggtheme = theme_bw(),
             alpha.ind = 0.5,
             addEllipses = TRUE,
             legend = "right")
```

```{r}
fviz_pca_ind(pca_DF, title = "PCA - Open Covid data - Ward's Heirarchical (n=4)", 
             habillage = subOpenCovid$hc_membership,
             palette = "jco",
             geom = "point",
             ggtheme = theme_bw(),
             alpha.ind = 0.5,
             addEllipses = TRUE,
             legend = "right")
```

```{r variable-wagon-wheel}
fviz_pca_var(pca_DF, col.var="steelblue")+
 theme_minimal()
```

